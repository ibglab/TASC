{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0918d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances, nan_euclidean_distances\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from fcmeans import FCM\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cmasher as cmr\n",
    "import timeit\n",
    "import random\n",
    "\n",
    "import lib.utils as u\n",
    "import lib.cluster_utils as cu\n",
    "import lib.linear_utils as lu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ebd883",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9904cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c10c29",
   "metadata": {},
   "source": [
    "# Load data and define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00479390",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = np.load(r'data/sim/sim_sig_5movs_5feats.npy')\n",
    "ground_boundaries = np.load(r'data/sim/sim_gtbound_5movs_5feats.npy')\n",
    "ground_labels = np.load(r'data/sim/sim_gtlabel_5movs_5feats.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9ad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f26853",
   "metadata": {},
   "source": [
    "# Calculating 'activity' periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bedbfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change threshold according to the amplitude of the 'movement' returned\n",
    "thresh = 0.015\n",
    "# change min length according to the time-scale of the quiet periods you observe\n",
    "min_length = 30\n",
    "quiet_periods, movement = u.find_quiet_periods(signal, sfreq=0.05, thresh=thresh, min_length=min_length, bord=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287e080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper-plot\n",
    "fig, ax = plt.subplots(figsize=(8,3), nrows=2, sharex=True)\n",
    "\n",
    "# change to reflect parts of the signal you are interested in\n",
    "t_min = max(0, 0)\n",
    "t_max = min(len(signal), 3_000)\n",
    "\n",
    "ax[0].plot(signal[t_min:t_max,-1], color='gray')\n",
    "for qs, qe in quiet_periods:\n",
    "    if qs < t_min: continue\n",
    "    if qe > t_max: break\n",
    "    ax[0].axvline(qs-t_min, linestyle='dotted', color='green')\n",
    "    ax[0].axvline(qe-t_min, linestyle='dotted', color='red');\n",
    "ax[0].set_title('Signal')\n",
    "\n",
    "ax[1].plot(movement[t_min:t_max], color='gray')\n",
    "ax[1].axhline(thresh, linestyle='dotted')\n",
    "ax[1].set_title('Movement (sum of low-passed feats)')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b16066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change win_size according to your data time-scale\n",
    "win_size = 80\n",
    "valid_starts = u.get_valid_starts(len(signal), quiet_periods, win_size, overlap=0)\n",
    "valid_periods = u.runs_of_ones(valid_starts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce924a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to reflect parts of the signal you are interested in\n",
    "t_min = max(0, 0)\n",
    "t_max = min(len(signal), 3_000)\n",
    "\n",
    "fig = plt.figure(figsize=(8,2))\n",
    "plt.title('Activity periods')\n",
    "plt.plot(signal[t_min:t_max,-1], color='blue', alpha=0.5)\n",
    "for qs, qe in valid_periods:\n",
    "    if qs < t_min:\n",
    "        continue\n",
    "    if qe > t_max:\n",
    "        break\n",
    "    plt.axvline(qs-t_min, linestyle='dotted', color='green')\n",
    "    plt.axvline(qe-t_min, linestyle='dotted', color='red');\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f523fc36",
   "metadata": {},
   "source": [
    "# Initial segmentation and clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd6404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize the step_size\n",
    "# PCA note: the more overlaps, the less informative are the pc's\n",
    "step_size = win_size//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e93b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect initial segmentation boundaries\n",
    "init_boundaries = []\n",
    "i = 0\n",
    "while i < len(signal)-win_size:\n",
    "    w = valid_starts[i:i+win_size]\n",
    "    if sum(w) > win_size * 0.95:\n",
    "        init_boundaries.append([i,i+win_size])\n",
    "        i += step_size\n",
    "    else:\n",
    "        i += 1\n",
    "init_boundaries = np.array(init_boundaries)\n",
    "print(init_boundaries.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303924dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumulate the segments\n",
    "init_segments = cu.get_segments(signal, init_boundaries, nan=False)\n",
    "print(init_segments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b11bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the nuumber to\n",
    "# either desired ratio of variance explained\n",
    "# or concerete number of componen\n",
    "n_components = 0.8\n",
    "# dimensionality reduction of the segments\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(np.r_[cu.flat_concat_segments(init_segments)])\n",
    "print(f'{pca.n_components_} components explain at least 0.8 of variance')\n",
    "init_segments_embed = pca.transform(cu.flat_concat_segments(init_segments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb2e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the number accordingly to data\n",
    "n_clusters = 5\n",
    "# cluster the dim.reduced segments\n",
    "m = cu.estimate_fuzzifier(init_segments_embed.shape[0], init_segments_embed.shape[1])\n",
    "fcm = FCM(n_clusters=n_clusters,m=m,random_state=seed)\n",
    "fcm.fit(init_segments_embed)\n",
    "# outputs\n",
    "init_centers = fcm.centers\n",
    "init_labels = fcm.predict(init_segments_embed)\n",
    "soft_labels = fcm.soft_predict(init_segments_embed)\n",
    "labels = np.max(soft_labels, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d94d7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the dense space\n",
    "# using the f-means scores\n",
    "t_start = timeit.default_timer()\n",
    "\n",
    "# filter_idx, filter_order = cu.fix_singular_overlaps(labels, init_boundaries)\n",
    "filter_idx = cu.divide_and_fix(labels, init_boundaries)\n",
    "\n",
    "t_end = timeit.default_timer()\n",
    "time_interval = (t_end-t_start)/60\n",
    "print(f'{time_interval:1.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc472e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = init_boundaries[filter_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eec5e7",
   "metadata": {},
   "source": [
    "# Visualize the initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328ac316",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = cmr.get_sub_cmap('Set1', 0, 0.8, N=n_clusters+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef8a803",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,4))\n",
    "\n",
    "# how many clusters to plot?\n",
    "n = 5\n",
    "n = min(n_clusters, np.unique(init_labels[filter_idx]).shape[0])\n",
    "n_shuffle = np.random.choice(np.unique(init_labels), n, replace=False)\n",
    "n_zip = np.array([[i,lab] for i,lab in enumerate(init_labels[filter_idx]) if lab in n_shuffle])\n",
    "n_idx, n_labels = n_zip.T\n",
    "\n",
    "plt.scatter(init_segments_embed[filter_idx][n_idx,0], init_segments_embed[filter_idx][n_idx,1], \n",
    "            c=n_labels, cmap=cmap, alpha=0.3)\n",
    "\n",
    "n_centers = np.array([ele for i,ele in enumerate(init_centers) if i in n_shuffle])\n",
    "plt.scatter(n_centers[:,0], n_centers[:,1], c=np.arange(n_clusters), marker='*', s=250, cmap=cmap)\n",
    "\n",
    "plt.title('Dim.reduced and clustered segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4949e719",
   "metadata": {},
   "source": [
    "Below: the rows span features and the columns - clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84433d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = signal.shape[1]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,5), ncols=n, nrows=n_features, squeeze=False)\n",
    "plt.suptitle('Samples of the clustered space')\n",
    "\n",
    "# note: assigning the filtered segments to 'init_segments'\n",
    "init_segments = cu.get_segments(signal, boundaries, nan=True)\n",
    "init_segments_centers = cu.get_centers(signal, boundaries, n_labels, mode='mean', real=True)\n",
    "\n",
    "n_segments = 20\n",
    "\n",
    "for c in range(n):\n",
    "    c_ind = np.where(n_labels==c)[0]\n",
    "    c_ind = np.random.choice(c_ind, min(len(c_ind), n_segments), replace=False)\n",
    "    for n in range(n_features):\n",
    "        ax[n,c].plot(init_segments[c_ind,:,n].T, color='gray', alpha=0.5)\n",
    "        ax[n,c].plot(init_segments_centers[c][:, n], color='blue')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aee67a",
   "metadata": {},
   "source": [
    "# TASC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf8cb23",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "1. Ground truth is present depending on the dataset. If none - you can exclude the fields from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687e437e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = ['silhouette', 'dunn']\n",
    "scores_dictionary = {'silhouette': 'up', 'dunn': 'up', 'avg_in': 'down', 'avg_out': 'up'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a047f54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tacom_history = {}\n",
    "tacom_history[0] = {'boundaries':boundaries, 'labels':n_labels, 'embed':init_segments_embed[filter_idx], \n",
    "                    'ground_boundaries':ground_boundaries,'ground_labels':ground_labels, \n",
    "                    'align_idx': np.arange(boundaries.shape[0]), 'centers': init_centers,\n",
    "                    'params': np.array([[0,1] for i in range(boundaries.shape[0])])}\n",
    "init_eval = cu.eval_clusters(tacom_history[0]['embed'], tacom_history[0]['labels'], eval_metrics)\n",
    "tacom_history[0]['evals'] = init_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d7d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be51b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_off = np.arange(-10, 11)\n",
    "len_off = np.arange(-10, 11)\n",
    "alpha = 0.5\n",
    "filtparam = 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dee7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tacom_call():\n",
    "    t_start = timeit.default_timer()\n",
    "    for n in range(n_epochs):\n",
    "        \n",
    "        print(f'Aligning space, epoch {n+1}')\n",
    "        align_boundaries, align_idx, gap_idx, align_params,*_ = cu.align_all_sequentially(\n",
    "            signal, tacom_history[n]['boundaries'], tacom_history[n]['labels'], valid_starts=valid_starts,\n",
    "            time_off=time_off, len_off=len_off, alpha=alpha, filtparam=filtparam) \n",
    "        \n",
    "        print('Embedding the space')\n",
    "        align_segments = cu.get_segments(signal, align_boundaries, nan=False)\n",
    "        align_segments[align_idx] = [lu.warp_linear(s,p, crop=True)[0] for s,p in zip(align_segments[align_idx], align_params[align_idx])]\n",
    "        # calculating the new average size of the segments\n",
    "        new_size = int(np.mean([b[1]-b[0] for b in align_boundaries[align_idx]]))\n",
    "        pca_align_segments = cu.flat_concat_segments(np.array([u.my_resample(e, 1, len(e), new_size) for e in align_segments]))\n",
    "        # we redefine the pca as we set it to explain\n",
    "        # n% of variance\n",
    "        pca = PCA(n_components=n_components)\n",
    "        align_segments_embed = pca.fit_transform(pca_align_segments)\n",
    "        \n",
    "        print('Cluster the space')\n",
    "        model = KMeans(n_clusters=n_clusters, n_init=10)\n",
    "        align_labels = model.fit_predict(align_segments_embed)\n",
    "\n",
    "        if len(gap_idx) > 0:\n",
    "            print('Aligning gaps')\n",
    "            _, _, _, cluster_sizes = cu.get_sizes(align_boundaries[align_idx], align_labels[align_idx])\n",
    "            centers = cu.get_centers(signal, align_boundaries, align_labels, cluster_sizes)\n",
    "            for gi in gap_idx:\n",
    "                gi_res = cu.score_temp_variants(signal, align_boundaries[gi], centers[align_labels[gi]],\n",
    "                                                np.arange(1), np.arange(1), alpha=alpha, valid_starts=valid_starts)\n",
    "                align_boundaries[gi] = gi_res[0][0]\n",
    "                align_params[gi] = gi_res[-1][0]\n",
    "\n",
    "            pca_align_segments[gap_idx] = cu.flat_concat_segments(np.array([u.my_resample(e, 1, len(e), new_size) for e in \n",
    "                                                np.array([lu.warp_linear(s,p, crop=True)[0] for s,p in \n",
    "                                                        zip(cu.get_segments(signal, align_boundaries[gap_idx], nan=False), \n",
    "                                                        align_params[gap_idx])], dtype=object)]))\n",
    "            align_segments_embed = pca.fit_transform(pca_align_segments)\n",
    "            align_labels = model.fit_predict(align_segments_embed)\n",
    "            \n",
    "        print('Evaluating and saving results')\n",
    "        evals = cu.eval_clusters(align_segments_embed, align_labels, metrics=eval_metrics)\n",
    "        for key,e in evals.items():\n",
    "            print(f'{key}: {e:.3f} ({scores_dictionary[key]})')\n",
    "\n",
    "        tacom_history[n+1] = {'boundaries':align_boundaries, 'labels':align_labels,\n",
    "                    'gap_idx':gap_idx, 'align_idx':align_idx, 'params':align_params,\n",
    "                       'evals': evals, 'embed': align_segments_embed, 'centers': model.cluster_centers_}\n",
    "\n",
    "    t_end = timeit.default_timer()\n",
    "    time_interval = (t_end-t_start)/60\n",
    "    print(f'{time_interval:1.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f445f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    tacom_call()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d643282",
   "metadata": {},
   "source": [
    "# Clustering evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5cd17a",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10a8d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_keys = tacom_history[0]['evals'].keys()\n",
    "nrows = len(evals_keys)//2\n",
    "ncols = len(evals_keys)//nrows\n",
    "fig,ax = plt.subplots(figsize=(8,8), nrows=nrows, ncols=ncols)\n",
    "ax = ax.flat\n",
    "for j,key in enumerate(evals_keys):\n",
    "    evals = [tacom_history[i]['evals'][key] for i in tacom_history.keys()]\n",
    "    ax[j].plot(evals)\n",
    "    ax[j].set_title(f'{key}')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6bbce6",
   "metadata": {},
   "source": [
    "## Clusters' progress in time domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb2d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize according to your needs\n",
    "feat = 0            # feature to plot\n",
    "n_show = [1]        # epochs to show\n",
    "n_segments = 50     # how many segments to plot in a single subplot\n",
    "\n",
    "def calc_average_dist(centers, members, sizes):\n",
    "    return np.mean(nan_euclidean_distances(cu.flat_concat_segments(cu.full_nans(members)[:,:int(sizes)]), \n",
    "                   cu.flat_concat_segments(np.array([u.my_resample(centers, 1, len(centers), int(sizes))]))))\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(8,10), nrows=1+2*(len(n_show)), ncols=n_clusters, sharex=True, sharey='row')\n",
    "\n",
    "    dist_reg = []\n",
    "    dist_bell = []\n",
    "    _,_,_,cluster_sizes = cu.get_sizes(tacom_history[0]['boundaries'], tacom_history[0]['labels'])\n",
    "    temp_segments = cu.get_segments(signal, tacom_history[0]['boundaries'], nan=True)\n",
    "    temp_centers = cu.get_centers(signal, tacom_history[0]['boundaries'], \n",
    "                                  tacom_history[0]['labels'], cluster_sizes, mode='mean', real=True)\n",
    "    for c in range(n_clusters):\n",
    "        c_ind = np.where(tacom_history[0]['labels']==c)[0]\n",
    "        c_ind = np.random.choice(c_ind, min(len(c_ind), n_segments), replace=False)\n",
    "        c_segments = temp_segments[c_ind]\n",
    "        for t in c_segments:\n",
    "            ax[0][c].plot(t[:,feat].T, color='gray', alpha=0.5)\n",
    "        ax[0][c].plot(np.nanmean(c_segments, axis=0)[:,feat], color='blue')\n",
    "        dist_reg.append(calc_average_dist(temp_centers[c], temp_segments, cluster_sizes[c]))\n",
    "        dist_bell.append(np.mean(u.weightedL2_all(c_segments, np.array(temp_centers[c]))))\n",
    "    dist_reg = np.mean(dist_reg)\n",
    "    dist_bell = np.mean(dist_bell)\n",
    "    ax[0][0].set_title(f'Initial guess; {dist_reg:.3f}; {dist_bell:.3f}')\n",
    "\n",
    "    for i,idx in enumerate(n_show):\n",
    "        _,_,_,cluster_sizes = cu.get_sizes(tacom_history[idx]['boundaries'], tacom_history[idx]['labels'])\n",
    "        temp_segments = cu.get_segments(signal, tacom_history[idx]['boundaries'], nan=True)\n",
    "        temp_centers = cu.get_centers(signal, tacom_history[idx]['boundaries'],\n",
    "                                      tacom_history[idx]['labels'], cluster_sizes, mode='mean', real=True)\n",
    "        dist_reg = []\n",
    "        dist_bell = []\n",
    "        for c in range(n_clusters):\n",
    "            c_ind = np.where(tacom_history[idx]['labels']==c)[0]\n",
    "            c_ind = np.random.choice(c_ind, min(len(c_ind), n_segments), replace=False)\n",
    "            c_segments = temp_segments[c_ind]\n",
    "            for t in c_segments:\n",
    "                ax[1+2*i][c].plot(t[:,feat], color='gray', alpha=0.5)\n",
    "            ax[1+2*i][c].plot(np.nanmean(c_segments, axis=0)[:,feat], color='blue')\n",
    "            dist_reg.append(calc_average_dist(temp_centers[c], temp_segments, cluster_sizes[c]))\n",
    "            dist_bell.append(np.mean(u.weightedL2_all(c_segments, np.array(temp_centers[c]))))\n",
    "        dist_reg = np.mean(dist_reg)\n",
    "        dist_bell = np.mean(dist_bell)\n",
    "        ax[1+2*i][0].set_title(f'Epoch {idx}; {dist_reg:.3f}; {dist_bell:.3f}')\n",
    "\n",
    "        dist_reg = []\n",
    "        dist_bell = []\n",
    "        temp_segments = cu.get_segments(signal, tacom_history[idx]['boundaries'], nan=False)\n",
    "        temp_segments = np.array([lu.warp_linear(t, tacom_history[idx]['params'][i], crop=False)[0] for i,t in enumerate(temp_segments)], dtype=object)\n",
    "        temp_centers = cu.get_centers(signal, tacom_history[idx]['boundaries'],\n",
    "                                      tacom_history[idx]['labels'], cluster_sizes, mode='mean', real=True)\n",
    "        for c in range(n_clusters):\n",
    "            c_ind = np.where(tacom_history[idx]['labels']==c)[0]\n",
    "            c_ind = np.random.choice(c_ind, min(len(c_ind), n_segments), replace=False)\n",
    "            c_segments = temp_segments[c_ind]\n",
    "            for t in c_segments:\n",
    "                ax[2+2*i][c].plot(t[:,feat], color='gray', alpha=0.5)\n",
    "            ax[2+i*2][c].plot(temp_centers[c][:,feat], color='blue')\n",
    "            dist_reg.append(calc_average_dist(temp_centers[c], temp_segments, cluster_sizes[c]))\n",
    "            dist_bell.append(np.mean(u.weightedL2_all(c_segments, np.array(temp_centers[c]))))\n",
    "        dist_reg = np.mean(dist_reg)\n",
    "        dist_bell = np.mean(dist_bell)\n",
    "        ax[2+i*2][0].set_title(f'Aligned, epoch {idx}; {dist_reg:.3f}; {dist_bell:.3f}')\n",
    "\n",
    "    plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c42406",
   "metadata": {},
   "source": [
    "## Clusters' progress in the dim. reduced space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb2265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_show = [0,1] # epochs to show\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,2), ncols=len(n_show), sharex=True, sharey=True)\n",
    "\n",
    "for e,n in enumerate(n_show):\n",
    "    temp_segments = cu.get_segments(signal, tacom_history[n]['boundaries'], nan=False)\n",
    "    temp_segments = [lu.warp_linear(s,p, crop=False)[0] for s,p in zip(temp_segments, tacom_history[n]['params'])]\n",
    "    # pca_align_segments = np.array([sig.resample(e, new_size).T.flatten() for e in align_segments])\n",
    "    # calculate the new_size\n",
    "    new_size, *_ = cu.get_sizes(tacom_history[n]['boundaries'], tacom_history[n]['labels'])\n",
    "    temp_segments = cu.flat_concat_segments(np.array([u.my_resample(e, 1, len(e), new_size) for e in temp_segments]))\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    temp_segments = pca.fit_transform(temp_segments)\n",
    "\n",
    "    ax[e].scatter(temp_segments[:,0], temp_segments[:,1], c=tacom_history[n]['labels'], cmap=cmap);\n",
    "    ax[e].set_title(f'Epoch {n}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bc98f9",
   "metadata": {},
   "source": [
    "# Segmentation evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac422bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_iou_scores = []\n",
    "overlap_indices = []\n",
    "overlap_labels = []\n",
    "overlap_new_labels = {}\n",
    "\n",
    "n_executed = n_epochs + 1\n",
    "\n",
    "segments_scores = {'Overlap':np.zeros(n_executed), 'Boundaries distance':np.zeros(n_executed), 'Boundaries success': np.zeros(n_executed),} \n",
    "#                    'Adjusted rand':np.zeros(n_executed), 'Homogeneity':np.zeros(n_executed), 'Completeness':np.zeros(n_executed),\n",
    "#                   'V Measure':np.zeros(n_executed), 'Fowlkes':np.zeros(n_executed)}\n",
    "\n",
    "y_true = tacom_history[0]['ground_boundaries']\n",
    "for e,idx in enumerate(np.arange(n_executed)):\n",
    "    y_pred = tacom_history[idx]['boundaries']\n",
    "    min_len = min(y_true.shape[0], y_pred.shape[0])\n",
    "    max_len = max(y_true.shape[0], y_pred.shape[0])\n",
    "    \n",
    "    # Find the indices of the ground truth segments that are significantly overlapped by the predicted segments\n",
    "    iou_scores = np.zeros((y_true.shape[0], y_pred.shape[0]))\n",
    "    for i in range(y_true.shape[0]):\n",
    "        for j in range(y_pred.shape[0]):\n",
    "            # each entry is overlap of a true boundary with a predicted boundary over actual boundary\n",
    "            # naturally the overlap is greater around the identity line\n",
    "            iou_scores[i,j] = (min(y_true[i, 1], y_pred[j, 1]) - max(y_true[i, 0], y_pred[j, 0]))/(y_true[i, 1] - y_true[i, 0])\n",
    "    overlap_iou_scores.append(iou_scores)\n",
    "    indices = []\n",
    "    for i,row in enumerate(iou_scores):\n",
    "        # only considers overlaps >0.5\n",
    "        # completely neglects the smaller overlaps in further computations...\n",
    "        if np.max(row) > 0.5: \n",
    "            indices.append([i, np.argmax(row)])\n",
    "    boundary_score = 0\n",
    "    boundary_success = 0\n",
    "    for ind in indices:\n",
    "        score = np.abs(y_true[ind[0]][0]-y_pred[ind[1]][0]) + np.abs(y_true[ind[0]][1]-y_pred[ind[1]][1])\n",
    "        boundary_score += score\n",
    "        if score < 0.2*(y_true[ind[0]][1]-y_true[ind[0]][0]):\n",
    "            boundary_success += 1\n",
    "    overlap_indices.append(indices)\n",
    "    segments_scores['Boundaries distance'][e] = boundary_score/len(y_true)\n",
    "    segments_scores['Boundaries success'][e] = boundary_success/len(y_true)\n",
    "\n",
    "    score = 0\n",
    "    for ind in indices:\n",
    "        score += iou_scores[ind[0], ind[1]]\n",
    "    score /= len(y_true)\n",
    "    segments_scores['Overlap'][e] = score\n",
    "    \n",
    "    overlap_label = np.zeros((len(indices), 2))\n",
    "    for i,ind in enumerate(indices):\n",
    "        overlap_label[i] = [tacom_history[0]['ground_labels'][ind[0]], tacom_history[idx]['labels'][ind[1]]]\n",
    "    overlap_labels.append(overlap_label)\n",
    "    \n",
    "    c_mat = confusion_matrix(overlap_label[:,0], overlap_label[:,1])\n",
    "    keys, values = linear_sum_assignment(c_mat.T, maximize=True)\n",
    "    mapping = dict(zip(keys, values))\n",
    "    temp_labels = [mapping[ele] for ele in overlap_label[:,1]]\n",
    "    overlap_new_labels[idx] = temp_labels\n",
    "    \n",
    "#     segments_scores['Adjusted rand'][e] = adjusted_rand_score(overlap_label[:,0], overlap_label[:,1])\n",
    "#     segments_scores['Homogeneity'][e], segments_scores['Completeness'][e], segments_scores['V Measure'][e] = homogeneity_completeness_v_measure(overlap_label[:,0], overlap_label[:,1])\n",
    "#     segments_scores['Fowlkes'][e] = fowlkes_mallows_score(overlap_label[:,0], overlap_label[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaac4e3",
   "metadata": {},
   "source": [
    "## Segmentation prediction vs the ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41efca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Overlap', 'Boundaries distance', 'Boundaries success']\n",
    "fig,ax = plt.subplots(figsize=(7,2), ncols=len(metrics))\n",
    "for e,key in enumerate(metrics):\n",
    "    ax[e].plot(segments_scores[key])\n",
    "    ax[e].set_title(key)\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a75b5",
   "metadata": {},
   "source": [
    "## Confusion matrices\n",
    "\n",
    "The matrices show progress of:\n",
    "1. Cluster separation\n",
    "2. Relationship between GT clusters and the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe588538",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_mat = confusion_matrix(tacom_history[0]['ground_labels'], tacom_history[0]['ground_labels'])/len(tacom_history[0]['ground_labels'])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=c_mat)\n",
    "disp.plot(values_format='.2f')\n",
    "plt.title('Ground')\n",
    "\n",
    "for idx in np.arange(n_executed):\n",
    "    temp_labels = overlap_new_labels[idx]\n",
    "    c_mat = confusion_matrix(overlap_labels[idx][:,0], temp_labels)/len(tacom_history[0]['ground_labels'])\n",
    "    if c_mat.shape != (n_clusters, n_clusters):\n",
    "        missing = [i for i in np.arange(n_clusters) if i not in np.unique(overlap_labels[idx][:,0])]\n",
    "        c_mat = np.insert(c_mat, missing, np.zeros(n_clusters), axis=0)\n",
    "\n",
    "    tp = sum([np.max(row) for row in c_mat])\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=c_mat)\n",
    "    disp.plot(values_format='.2f')\n",
    "    plt.title(f'{idx}; tp={tp:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e57dd",
   "metadata": {},
   "source": [
    "## Direct segmentation comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = u.gen_colors(n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6088d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_show = [0] + [1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,4), nrows=1+len(n_show), sharex=True)\n",
    "\n",
    "fro, up_to = 0,1_200\n",
    "\n",
    "for i,b in enumerate(tacom_history[0]['ground_boundaries']):\n",
    "    if b[0] < fro: continue\n",
    "    if b[1] >= up_to: break\n",
    "    color = colors[tacom_history[0]['ground_labels'][i]]\n",
    "    ax[0].fill_between(b, 1, color=color, alpha=0.5)\n",
    "    ax[0].axvline(b[0], linestyle='dotted', color='black')\n",
    "    for a in ax[1:]:\n",
    "        a.axvline(b[0], linestyle='--', color='gray')\n",
    "        a.axvline(b[1], linestyle='--', color='gray')\n",
    "ax[0].plot(np.arange(fro, up_to), signal[fro:up_to,-1], color='black', alpha=0.5)\n",
    "\n",
    "for e,idx in enumerate(n_show):\n",
    "    if idx == 0:\n",
    "        temp_labels = tacom_history[idx]['labels']\n",
    "    else:\n",
    "        temp_labels = overlap_new_labels[idx]\n",
    "    for i,b in enumerate(tacom_history[idx]['boundaries']):\n",
    "        if b[0] < fro: continue\n",
    "        if b[1] >= up_to: break\n",
    "        color = colors[temp_labels[i]]\n",
    "        ax[1+e].fill_between(b, 1, color=color, alpha=0.5)\n",
    "        ax[1+e].axvline(b[0], linestyle='dotted', color='black')\n",
    "        ax[1+e].axvline(b[1], linestyle='dotted', color='black')\n",
    "    ax[1+e].set_title(f'Epoch {idx}')\n",
    "    ax[1+e].plot(np.arange(fro, up_to), signal[fro:up_to,-1], color='black', alpha=0.5)\n",
    "\n",
    "plt.tight_layout();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
